{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Exploratory Data Analysis and Unsupervised Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an important step in the Data Science process. EDA involves the investigation of a dataset using various graphical and statistical techniques in order to identify underlying relationships, characteristics, and patterns within the data.\n",
    "\n",
    "<br/>\n",
    "\n",
    "When performing EDA, we aim to answer questions such as:\n",
    "   * how many observations and features are present in the dataset?\n",
    "   * is the data labelled or unlabelled?\n",
    "   * does the dataset consist of time-series, sequence, or independent data points?\n",
    "   * how is the feature data distributed?\n",
    "   * are the features numerical, categorical, or both?\n",
    "   * is there evidence of correlation between features?\n",
    "   * is the dataset balanced?\n",
    "   * will you need to use encoding to convert categorical features to numerical features?\n",
    "   * does the dataset have missing or null values?\n",
    "   * how will missing or null values be handled?\n",
    "   * are there obvious outliers in the data?\n",
    "   * will dimensionality reduction be required for the dataset?\n",
    "   * should the data be normalized?\n",
    "\n",
    "<br/>\n",
    "\n",
    "Although this lab should not be considered an exhaustive review of EDA, the following exercises will provide you with the opportunity to work through several example EDA techniques, answering many of the above questions in the process.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Exercise 1 - Exploratory Data Analysis\n",
    "\n",
    "First, we need to define our imports - the set of libraries that will help us to explore our dataset.\n",
    "\n",
    "For this notebook, our imports can be organized into 3 categories:\n",
    "   * **numerical calculation and data analysis:** pandas, numpy\n",
    "   * **visualization:** matplotlib, seaborn\n",
    "   * **machine learning:** sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Begin by loading a test dataset - in this case, [Fisher's iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), a famous multivariate dataset originally published in 1936 by Ronald Fisher.\n",
    "\n",
    "For convenience, this dataset is included with the scikit-learn library, and is a popular dataset for exploring machine learning concepts such as clustering and classification.\n",
    "\n",
    "The iris dataset can be loaded using the *load_iris()* helper method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** When loading a custom dataset, you obviously will not have access to such a helper method.\n",
    "\n",
    "For future reference, custom datsets can easily be loaded directly into a pandas DataFrame using a command such as:\n",
    "> df = pd.read_csv(\"/path/to/your/file.csv\")\n",
    "\n",
    "See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "\n",
    "We will cover *pandas* and *DataFrames* later in this lab.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Invoking a variable will often provide useful information. Let's try this for our *iris* variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "By inspecting the output, we see that the *iris* variable references a dictionary-like data structure that contains several useful elements:\n",
    "\n",
    "| Element | Purpose |\n",
    "|---|:---|\n",
    "| DESCR | contains a plain-text description of the dataset |\n",
    "| data | contains an array of data, with rows representing individual observations, and columns representing features |\n",
    "| feature_names | contains a list of our feature names |\n",
    "| filename | shows the local file system path of the dataset data file |\n",
    "| target | contains an array of the class IDs for each of our observations |\n",
    "| target_names | contains the target class labels |\n",
    "\n",
    "<br/>\n",
    "\n",
    "In this lab, we will make use of *data*, *target*, *feature_names*, and *target_names*.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Let's take a few moments to explore the *data* element of *iris*. This can be achieved by calling `iris.data` or `iris['data']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " iris['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Next, let's determine the dimensions of the *iris.data* array by calling `iris.data.shape`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We can see that the *iris.data* array has *150* rows and *4* columns.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, let's examine the list of feature names via `iris.feature_names`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "For the iris dataset, the 4 features consist of measurements of sepal and petal widths & lengths.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, let's take a look at the `iris.target` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The *iris.target* array maps each of the observations from our *iris.data* array to one of three target IDs: 0, 1, or 2.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** For today's exercise, it is very helpful for us to use a dataset that contains the target ID (known label) for each observation. In practice, you will not always have the luxury of working with a dataset with known labels.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "The numerical target IDs can be mapped to their corresponding text equivalent via the `iris.target_names` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output:\n",
    "   * ID 0 represents 'setosa'\n",
    "   * ID 1 represents 'versicolor'\n",
    "   * ID 2 represents 'virginica'\n",
    "   \n",
    "These are the 3 species of iris found in our dataset.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Before we begin analyzing the dataset, we will first load the data into a pandas DataFrame.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "If you are not familiar with pandas, it is \"an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\" (https://pandas.pydata.org/)\n",
    "\n",
    "The pandas libray is widely used for data exploration and analysis.\n",
    "\n",
    "pandas has 2 main types of data structures:\n",
    "   * **Series** a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.)\n",
    "   * **DataFrame** a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object.\n",
    "\n",
    "We will use both types of pandas data structures in today's exercises.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "Let's begin by creating a new DataFrame called *df*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "You can examine your newly created DataFrame by calling `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "You will notice that the pandas DataFrame provides an easier to read representation of the data, as compared to the *iris.data* array output that we examined in an earlier step.\n",
    "\n",
    "<br/>\n",
    "\n",
    "You can also use the DataFrame *head()* helper method to show the first 5 rows of data in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "As it stands, the column names in our DataFrame are not particularly useful. Let's replace the numerical IDs with the feature names from our dataset, and re-run `df.head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = iris.feature_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Compared to the previous output, this is much better!\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, let's use some additional DataFrame helper methods to see if there are any *null* values in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Similarly, let's see if there are any *missing* values in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Examining the above output, you will see that the iris dataset does not have any missing/null values. This is convenient for today's exercise, but perhaps not the norm.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** In practice,  whenever you encounter null/missing values in a dataset, you will need to make a decision on how to handle those values.\n",
    "\n",
    "For example, you might choose to:\n",
    "   * drop the affected rows or columns\n",
    "   * impute the missing values (ex: replace each missing value with the mean value within the given feature)\n",
    "   * interpolate the missing values based on adjacent values in the dataset\n",
    "   * leverage a downstream machine learning algorithm that is robust to missing values\n",
    "   \n",
    "For additional details, see https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "Moving on, let's add the corresponding numerical target IDs (representing species) to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'] = pd.Series(iris.target)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the above output, you will notice a new column called 'species' has been added to our DataFrame, consisting of a pandas Series that was created using the values in the *iris.target* array.\n",
    "\n",
    "<br/>\n",
    "\n",
    "To make the *species* column easier to understand, we will replace the numerical target IDs in the *species* column with the actual species names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['species'] = df.species.map(pd.Series(iris.target_names))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Things are starting to come together!\n",
    "\n",
    "<br/>\n",
    "\n",
    "Now that we have all of the iris data in a DataFrame, let's explore some additional DataFrame helper methods.\n",
    "\n",
    "First, let's try `df.info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame *info()* method provides a concise overview of the contents of the DataFrame.\n",
    "\n",
    "In the output above, we can quickly see that the iris dataset consists of 150 observations each containing 4 *numerical* features, plus the *species* categorical feature that we just added.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Many machine learning algorithms (including the ones we will use today) require numerical features as input. If your dataset contains non-numerical (categorical) features that we want to include in our analysis, we would often choose to encode the categorical features into numerical features using a process called *feature engineering*.\n",
    "\n",
    "Examples include [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and [label encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder).\n",
    "\n",
    "For an explanation of when to use *one-hot encoding* versus *label encoding*, refer to the following [article](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621).\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "We will be exluding the *species* column from our machine learning analyses, so we will not need to perfrom *feature engineering* today.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, let's try `df.describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The DataFrame *describe()* method is a quick & easy way to capture basic statistics for all numerical columns in a DataFrame.\n",
    "\n",
    "<br/>\n",
    "\n",
    "With DataFrames, you can easily perform aggregations. Let's say you are interested in the quantity of observations associated with each species in our dataset. This can be accomplished using the *groupby()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "You will notice that in our case, each of the 3 species classes contain the same number of observations (50 each). We consider this dataset balanced (an ideal situation).\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** In practice, you will encounter datasets for which the classes are not balanced - a situation that can have implications for machine learning algorithms. Please refer to [this article](https://medium.com/james-blogs/handling-imbalanced-data-in-classification-problems-7de598c1059f) to explore methods for dealing with unbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, let's further explore the dataset...\n",
    "\n",
    "In many cases, insights can be gained by investigating the distribution of feature data within a dataset. A commonly used tool for visualizing distributions is the [histogram](https://en.wikipedia.org/wiki/Histogram).\n",
    "\n",
    "Conveniently, the matplotlib library provides an easy to use *hist()* method to generate a histogram for a given DataFrame.\n",
    "\n",
    "Let's create a histogram for the 'sepal length' feature by calling `df['sepal length (cm)'].hist(bins=40)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal length (cm)'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right?\n",
    "\n",
    "Repeat the same procedure for the other 3 features, and compare the distributions. Feel free to adjust the value of the *bins* parameter, and observe how it changes the histograms.\n",
    "\n",
    "<br/>\n",
    "\n",
    "With a few extra lines of code, we can easily visualize and compare the distributions of all 4 features (sepal length, sepal width, petal length, petal width) at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(12,8))\n",
    "fig.suptitle(\"Distribution of sepal/petal lengths and widths\", fontsize=18)\n",
    "h1 = axs[0].hist(df['sepal length (cm)'], bins=20)\n",
    "h2 = axs[1].hist(df['sepal width (cm)'], bins=20, color='r')\n",
    "h3 = axs[2].hist(df['petal length (cm)'], bins=20, color='g')\n",
    "h4 = axs[3].hist(df['petal width (cm)'], bins=20, color='c')\n",
    "fig.legend(loc='center right', fontsize=13)\n",
    "plt.subplots_adjust(right=0.78, top=0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In the above histograms, you will see that sepal length and sepal width appear to be [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution), whereas petal length and petal width exhibit [bimodal distributions](https://en.wikipedia.org/wiki/Multimodal_distribution).\n",
    "\n",
    "<br/>\n",
    "\n",
    "The [density plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) is a similar method for visualizing data distributions.\n",
    "\n",
    "Let's create a combined density plot for all 4 features using the *kdeplot()* method from the seaborn library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sns.kdeplot(df['sepal length (cm)'], shade=True, bw='scott')\n",
    "sns.kdeplot(df['sepal width (cm)'], shade=True, bw='scott')\n",
    "sns.kdeplot(df['petal length (cm)'], shade=True, bw='scott')\n",
    "sns.kdeplot(df['petal width (cm)'], shade=True, bw='scott')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Again, note that *sepal length* and *sepal width* appear to be normally distributed, while *petal length* and *petal width* show bimodal distributions.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In the source code for the above density plot, the bandwidth *(bw)* value was estimated using Scott's Rule *(D.W. Scott, “Multivariate Density Estimation: Theory, Practice, and Visualization”, John Wiley & Sons, New York, Chicester, 1992.)*.\n",
    "\n",
    "The choice of bandwidth value will affect the overall smoothing of the density plot. It is sometimes useful to choose a custom value for *bw* in order to further investigate the underlying distribution.\n",
    "\n",
    "Let's change the value of *bw* and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sns.kdeplot(df['sepal length (cm)'], shade=True, bw=0.1)\n",
    "sns.kdeplot(df['sepal width (cm)'], shade=True, bw=0.1)\n",
    "sns.kdeplot(df['petal length (cm)'], shade=True, bw=0.1)\n",
    "sns.kdeplot(df['petal width (cm)'], shade=True, bw=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in smoothing between this density plot and the one generated in the previous step.\n",
    "\n",
    "Try re-generating the above density plots using several different values of *bw*. Note how *bw* affects the appearance of the density plots.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "When you are exploring a dataset with multiple features, it is often interesting to plot the features against one another to see if there are any noticeable relationships or structures.\n",
    "\n",
    "The seaborn library has a helpful method called *pairplot()* that conveniently handles all of the heavy lifting for us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df,palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "In the above plot, the upper-left to lower-right diagonal tiles depict histograms for the 4 features in the iris dataset. The surrounding tiles show scatterplots for all pairwise combinations of the 4 features in our dataset.\n",
    "\n",
    "Notice that in all of the scatterplots, you can clearly see 2 clusters present.\n",
    "\n",
    "In practice, you will often find yourself exploring \"unlabelled\" data that does not have any target labels associated with the individual observations (much like we see in the above pairplot). Looking at the above pairplot, we can not determine which data points are associated with which class in our dataset. We can see the presence of 2 clusters, but the class assignment of the data points is unknown.\n",
    "\n",
    "For this exercise, we actually do have labelled data. Let's repeat the *pairplot()*, this time coloring each observation based on its known target label (species):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df,hue='species',palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the above scatterplots, you can clearly observe that that the data points for a given species tend to fall within the same general region in the scatterplots. You will also see that the the data points are organized into 2 reasonably well defined clusters. Data points associated with *setosa* are easily separable from *versicolor* and *virginica*, whereas *versicolor* and *virginica* overlap to some degree.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** It is worth mentioning that the iris dataset has a relatively small number of features, and can easily be visualized in a pairplot. For datasets with more than a handful of features, you will often benefit from leveraging some form of dimensionality reduction, such as [Principal Component Analysis](https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis_(PCA)).\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "Let's finish off this section with 2 more examples of methods for visualizing data distributions: 1) the boxplot, and 2) the violin plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.violinplot(data=df, orient='h', palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these plots to your previous histograms and density plots. Do you notice any outliers?\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "# Exercise 2 - Unsupervised Learning (K-means Clustering)\n",
    "\n",
    "<br/>\n",
    "\n",
    "In this section, we will apply unsupervised learning to the iris dataset. Specifically, we will apply the [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) algorithm to the dataset to attempt to organize the data points into a set number of clusters.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Please keep in mind that unsupervised learning is usually applied to unlabelled datasets. In our case, we know that the iris dataset contains target labels. For the purpose of this analysis, however, we will exclude the labels (species) from the clustering process, simulating the analysis of an unlabelled dataset.\n",
    "\n",
    "So for today - let's pretend that the iris dataset contains unlabelled data.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "Begin by creating a new DataFrame *(df2)* from our original DataFrame, and removing the *species* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns = 'species')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have now removed our target labels *(species)*, and now have an unlabelled dataset.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Often times, you will need to normalize your dataset before applying a given machine learning algorithm. In general, the k-means algorithm will usually benefit from normalization, so we will perform the normalization process here.\n",
    "\n",
    "Let's use the *StandardScaler* method from the sklearn library to normalize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df2[df2.columns] = scaler.fit_transform(df2[df2.columns])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset values have been scaled from their original values.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Before we apply the k-means algorithm to our dataset, we need to choose an appropriate value for _k_, which represents the number of clusters that will be produced by the k-means algorithm. In our case, we already know that there are 3 species of iris in the dataset, and so '3' would obviously be a great choice for *k*.\n",
    "\n",
    "In practice, though, you will not usually know the number of clusters in advance.\n",
    "\n",
    "<br/>\n",
    "\n",
    "So - how should you choose an appropriate value of *k* when the number of classes is not known in advance?\n",
    "\n",
    "<br/>\n",
    "\n",
    "Luckily there are several methods for choosing a reasonably good value for *k*.\n",
    "\n",
    "Below, we will use the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), which begins by applying k-means to the dataset over a range of values of _k_ and recording the [*inertia*](https://scikit-learn.org/stable/modules/clustering.html) for each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-\n",
    "# for-k-means-clustering-14f27070048f\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "\n",
    "K = range(1,15)\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df2)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Next, we will plot the various *inertia* values, in terms of their corresponding *k* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-\n",
    "# for-k-means-clustering-14f27070048f\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the elbow method, the idea is to choose the _k_ value at which the *inertia* or *sum of squared distances* begins to level off.\n",
    "\n",
    "In the above graph, you can see that the curve levels off around *k* == 3.\n",
    "\n",
    "We will therefore apply the k-means algorithm with a *k* value of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster = KMeans(n_clusters=3)\n",
    "km = km_cluster.fit(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now that we have applied the *k-means* algorithm, let's take a look at the cluster assignment that was produced by the algorithm.\n",
    "\n",
    "The cluster assignment (labels) are available via `km.labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you see that the *k-means* algorithm has provided us with an array of 150 labels, corresponding to the 150 observations in our unlabelled dataset stored in DataFrame *df2*.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Let's take a few moments to add the cluster labels to our *df2* DataFrame.\n",
    "\n",
    "First, let's display the first 5 rows of our DataFrame (for comparison):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add a new 'cluster' column to the DataFrame, using the values from *km.labels_*.\n",
    "\n",
    "To make it easier to understand the 'cluster' column, we map the numerical values (0, 1, 2) to textual values (cluster1, cluster2, cluster3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['cluster'] = pd.Series(km.labels_)\n",
    "df2['cluster'] = df2.cluster.map(pd.Series(['cluster1','cluster2','cluster3']))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our cluster labels in place, let's do a *pairplot()*, coloring the data points based on their cluster assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df2,hue='cluster',palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice a striking similarity to the *pairplot()* plots of our original (species-labelled) dataset, generated earlier on in this lab.\n",
    "\n",
    "From visual inspection, the *k-means* algorithm has done a fair job at organizing the unlabelled data points into 3 clusters, which appear to represent our 3 iris species.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Let's take a closer look, and plot *petal width* versus *petal length* for both our original labelled dataset (our ground truth), and the new clustered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"petal width (cm)\", y=\"petal length (cm)\", data=df, fit_reg=False, hue='species', legend=True)\n",
    "sns.lmplot(x=\"petal width (cm)\", y=\"petal length (cm)\", data=df2, fit_reg=False, hue='cluster', legend=True, \\\n",
    "           palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the above plots, you will see that the *k-means* results are good, but not perfect. The *setosa* data points have been clustered with high accuracy, but there are obviously data points for *versicolor* and *virginica* that have been assigned to the incorrect cluster.\n",
    "\n",
    "Considering that *k-means* is an unsupervised approach, these results are still quite promising.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "By comparing our known *species* labels with our *k-means* cluster labels in a frequency table, we can determine which cluster represents which species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct1 = pd.crosstab(df['species'],df2['cluster'])\n",
    "sns.heatmap(ct1,annot=True,cbar=False,cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, you will see that one cluster represents all 50 of the *setosa* data points, one cluster represents mostly *versicolor* data points, and the other cluster represents mostly *virginica* data points.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Based on the above results, even if the iris dataset did not contain the target/species labels, we could use the *k-means algorithm* to organize the unlabelled data points into 3 clusters, roughly representing the 3 iris species.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Consider how this method could be useful when you are approaching unlabelled datasets.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "# Exercise 3 - Unsupervised Learning (Hierarchical Clustering)\n",
    "\n",
    "In this final section, you will repeat the unsupervised learning process, this time by applying a [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) algorithm to unlabelled iris data.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Begin by creating a new DataFrame *(df3)* from DataFrame *(df2)*, and dropping the *cluster* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(columns='cluster')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that we are starting with an unlabelled dataset - much like you will encounter while analyzing real-world datasets in practice.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Similar to the the *k-means* algorithm, our *hierarchical clustering* algorithm also requires a parameter that indicates that number of clusters that will be generated from our data.\n",
    "\n",
    "---\n",
    "\n",
    "As we have already performed the *elbow method* in a previous step, we will reuse the discovered value (3) here rather than repeating the process.\n",
    "\n",
    "---\n",
    "\n",
    "Let's apply the hierarchical clustering algorithm to our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "hc = hc_cluster.fit(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Our cluster assignment labels can be viewed via `hc.labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Next we will add the cluster assignment labels as a new 'cluster' column in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['cluster'] = pd.Series(hc.labels_)\n",
    "df3['cluster'] = df3.cluster.map(pd.Series(['cluster1','cluster2','cluster3']))\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "As before, we will generate a *pairplot()*, coloring each data point based on its cluster assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df3,hue='cluster',palette=\"Set2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this *pairplot()* to the previous *pairplot()* from the *k-means* results.\n",
    "\n",
    "Do these results look better or worse than those from the *k-means* exercise?\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Lastly, let's generate another frequency table comparing our known species labels to the *hierarchical clustering* labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct1 = pd.crosstab(df['species'],df3['cluster'])\n",
    "sns.heatmap(ct1,annot=True,cbar=False,cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above table, the *setosa* data points are nearly perfectly assigned to one cluster (49/50).\n",
    "\n",
    "Compared to the results from the *k-means* analysis, the *hierarchical clustering* algorithm did not cluster the *versicolor* and *virginica* data points quite as accurately.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "In machine learning, keep in mind the notion of the [no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem).\n",
    "\n",
    "Essentially - no single ML algorithm will be optimal for every data set. \n",
    "\n",
    "Although *k-means* appears to outperform *hierarchical clustering* for the iris dataset, the opposite could be true on an unrelated dataset.\n",
    "\n",
    "So - learn about the various unsupervised learning algorithms, their parameters, and be sure to experiment!\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "For an interesting comparison of clustering algorithms, see: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "Created by Scott Perry (perrysc@amazon.com) 20190324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
