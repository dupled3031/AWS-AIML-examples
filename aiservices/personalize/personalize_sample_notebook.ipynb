{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalize example\n",
    "\n",
    "Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\n",
    "\n",
    "Machine learning is being increasingly used to improve customer engagement by powering personalized product and content recommendations, tailored search results, and targeted marketing promotions. However, developing the machine-learning capabilities necessary to produce these sophisticated recommendation systems has been beyond the reach of most organizations today due to the complexity. Amazon Personalize allows developers with no prior machine learning experience to easily build sophisticated personalization capabilities into their applications, using machine learning technology perfected from years of use on Amazon.com.\n",
    "\n",
    "With Amazon Personalize, you provide an activity stream from your application – clicks, page views, signups, purchases, and so forth – as well as an inventory of the items you want to recommend, such as articles, products, videos, or music. You can also choose to provide Amazon Personalize with additional demographic information from your users such as age, or geographic location. Amazon Personalize will process and examine the data, identify what is meaningful, select the right algorithms, and train and optimize a personalization model that is customized for your data. All data analyzed by Amazon Personalize is kept private and secure, and only used for your customized recommendations. You can start serving personalized recommendations via a simple API call. You pay only for what you use, and there are no minimum fees and no upfront commitments.\n",
    "\n",
    "Amazon Personalize is like having your own Amazon.com machine learning personalization team at your disposal, 24 hours a day.\n",
    "\n",
    "In this example we will pull down the opensource movielens dataset. We will import this into Personalize, and then use the model to make personalized recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Personalize-how-it-works.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boto3 Personalize APIs.\n",
    "\n",
    "there are three Personalize APIs. Each play a different role. \n",
    "\n",
    "### Boto3 Personalize\n",
    "\n",
    "This is the API where you configure your data sets, define your model, select the algorithm and kick off training.\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html\n",
    "\n",
    "### Boto3 Personalize Runtime   \n",
    "    \n",
    "With this API you can provide a user id or item id, and get back recommendations.\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize-runtime.html\n",
    "\n",
    "### Boto3 Personalize Events\n",
    "\n",
    "With this API, you can send new user intercation events back to Personalize whjich gets used for further training.\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize-events.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The steps in the example are as follows\n",
    "\n",
    "![title](Personalize-steps-apis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the modules we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import JSON\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we created a Personalize client which we will use across this notebook\n",
    "We also set a timer so that we can track the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBucket(bucketname):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_buckets()\n",
    "    existingbuckets = [d['Name'] for d in response[\"Buckets\"]]\n",
    "    #print(existingbuckets)\n",
    "    if bucketname not in existingbuckets:\n",
    "        print(\"creating bucket \" + bucketname)\n",
    "        s3.create_bucket(Bucket=bucketname)\n",
    "    else:\n",
    "        print(\"bucket exists! \" + bucketname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Specify a Bucket and Data Output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket exists! aimlbootcamp247322960887\n"
     ]
    }
   ],
   "source": [
    "accountid = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = \"aimlbootcamp\" + accountid\n",
    "\n",
    "createBucket(bucket)\n",
    "\n",
    "#bucket = \"personalize-demo\"       # replace with the name of your S3 bucket\n",
    "filename = \"movie-lens-100k.csv\"  # replace with a name that you want to save the dataset under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Prepare, and Upload Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-23 12:57:41--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘ml-100k.zip’ not modified on server. Omitting download.\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>13</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>882399156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>879959583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       USER_ID  ITEM_ID  RATING  TIMESTAMP\n",
       "0          196      242       3  881250949\n",
       "1          186      302       3  891717742\n",
       "...        ...      ...     ...        ...\n",
       "99998       13      225       2  882399156\n",
       "99999       12      203       3  879959583\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -N http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip\n",
    "data = pd.read_csv('./ml-100k/u.data', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "pd.set_option('display.max_rows', 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['RATING'] > 3.6]                # keep only movies rated 3.6 and above\n",
    "data = data[['USER_ID', 'ITEM_ID', 'TIMESTAMP']] # select columns that match the columns in the schema below\n",
    "data.to_csv(filename, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(filename).upload_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will do the parts highlighted in red\n",
    "\n",
    "![title](Personalize-create-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schema\n",
    "\n",
    "First we need to create a schema that describes the data we are importing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createschema(schema, name):\n",
    "    \n",
    "    response = personalize.list_schemas(\n",
    "        maxResults=100\n",
    "    )\n",
    "\n",
    "    #print(\"response: \", response)\n",
    "    \n",
    "    for item in response[\"schemas\"]:\n",
    "        if item[\"name\"] == name:\n",
    "            return item[\"schemaArn\"]\n",
    "\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = name,\n",
    "        schema = json.dumps(schema)\n",
    "    )\n",
    "\n",
    "    schema_arn = create_schema_response['schemaArn']\n",
    "    #print(json.dumps(create_schema_response, indent=2))\n",
    "    return schema_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_arn:  arn:aws:personalize:us-east-1:247322960887:schema/aimlbootcamp-schema-personalize-20191102\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "schema_arn = createschema(schema, \"aimlbootcamp-schema-personalize-20191102\")\n",
    "print(\"schema_arn: \", schema_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group\n",
    "\n",
    "We create a dataset group next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdatasetGroup(name):\n",
    "    response = personalize.list_dataset_groups(\n",
    "        maxResults=100\n",
    "    )\n",
    "    print(\"response: \", response)\n",
    "    \n",
    "    for item in response[\"datasetGroups\"]:\n",
    "        if item[\"name\"] == name:\n",
    "            return item[\"datasetGroupArn\"]\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = name\n",
    "    )\n",
    "\n",
    "    dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    #print(json.dumps(create_dataset_group_response, indent=2))   \n",
    "    return dataset_group_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:  {'datasetGroups': [{'name': 'aimlbootcamp-20191102', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/aimlbootcamp-20191102', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 12, 27, 18, 37, 18, 797000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 12, 27, 18, 37, 39, 933000, tzinfo=tzlocal())}, {'name': 'aimlbootcamp-20200122', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/aimlbootcamp-20200122', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2020, 1, 22, 14, 44, 25, 300000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2020, 1, 22, 14, 44, 27, 4000, tzinfo=tzlocal())}, {'name': 'danielc', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/danielc', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 9, 1, 3, 37, 13, 569000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 9, 1, 3, 37, 41, 721000, tzinfo=tzlocal())}, {'name': 'danield', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/danield', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 9, 1, 3, 53, 53, 660000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 9, 1, 3, 54, 16, 928000, tzinfo=tzlocal())}, {'name': 'danieltest', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/danieltest', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 8, 11, 18, 7, 52, 696000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 8, 11, 18, 8, 3, 762000, tzinfo=tzlocal())}, {'name': 'test', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/test', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2020, 1, 23, 2, 20, 24, 720000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2020, 1, 23, 2, 20, 46, 788000, tzinfo=tzlocal())}, {'name': 'test20190831', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/test20190831', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 9, 1, 3, 13, 58, 891000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 9, 1, 3, 14, 15, 97000, tzinfo=tzlocal())}, {'name': 'try2', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/try2', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 9, 1, 4, 8, 35, 813000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 9, 1, 4, 8, 55, 509000, tzinfo=tzlocal())}, {'name': 'try3', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/try3', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 9, 1, 13, 7, 37, 337000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 9, 1, 13, 8, 6, 961000, tzinfo=tzlocal())}, {'name': 'try5', 'datasetGroupArn': 'arn:aws:personalize:us-east-1:247322960887:dataset-group/try5', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2019, 9, 1, 13, 28, 19, 533000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2019, 9, 1, 13, 28, 36, 62000, tzinfo=tzlocal())}], 'ResponseMetadata': {'RequestId': '6ec8956a-a576-4bef-be1f-ff4ca243b1a9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1', 'date': 'Thu, 23 Jan 2020 12:57:42 GMT', 'x-amzn-requestid': '6ec8956a-a576-4bef-be1f-ff4ca243b1a9', 'content-length': '2033', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "dataset_group_arn:  arn:aws:personalize:us-east-1:247322960887:dataset-group/aimlbootcamp-20200122\n"
     ]
    }
   ],
   "source": [
    "datasetPrefix = \"aimlbootcamp-20200122\"\n",
    "\n",
    "dataset_group_arn = createdatasetGroup(datasetPrefix)\n",
    "print(\"dataset_group_arn: \", dataset_group_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataset(name, dataset_type, dataset_group_arn, schema_arn):\n",
    "    response = personalize.list_datasets(\n",
    "        datasetGroupArn=dataset_group_arn,\n",
    "        maxResults=100\n",
    "    )\n",
    "    \n",
    "    #print(\"response: \", response)\n",
    "    for item in response[\"datasets\"]:\n",
    "        print(\"inspecting: \", item)\n",
    "        if item[\"name\"] == name or item[\"datasetType\"] == dataset_type:\n",
    "            #return item[\"datasetArn\"]\n",
    "            response = personalize.delete_dataset(\n",
    "                datasetArn=item[\"datasetArn\"]\n",
    "            )\n",
    "            max_time = time.time() + 2*60 # 10 minutes\n",
    "            while time.time() < max_time:\n",
    "                try:\n",
    "                    response = personalize.describe_dataset(datasetArn=item[\"datasetArn\"])\n",
    "                except Exception as e:\n",
    "                    if \"ResourceNotFoundException\".lower() in str(e).lower():\n",
    "                        print(\"delete completed\")\n",
    "                        break\n",
    "                except:\n",
    "                    raise\n",
    "                                    \n",
    "                status = response[\"dataset\"][\"status\"]\n",
    "                print(\"DatasetGroup: {}\".format(status))\n",
    "\n",
    "                time.sleep(5)\n",
    "            \n",
    "            \n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = schema_arn\n",
    "    )\n",
    "    print(\"dataset created....\")\n",
    "    dataset_arn = create_dataset_response['datasetArn']\n",
    "    #print(json.dumps(create_dataset_response, indent=2))\n",
    "    return dataset_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspecting:  {'name': 'aimlbootcamp-20200122-dataset', 'datasetArn': 'arn:aws:personalize:us-east-1:247322960887:dataset/aimlbootcamp-20200122/INTERACTIONS', 'datasetType': 'INTERACTIONS', 'status': 'ACTIVE', 'creationDateTime': datetime.datetime(2020, 1, 23, 2, 38, 35, 367000, tzinfo=tzlocal()), 'lastUpdatedDateTime': datetime.datetime(2020, 1, 23, 2, 38, 35, 367000, tzinfo=tzlocal())}\n",
      "DatasetGroup: DELETE PENDING\n",
      "DatasetGroup: DELETE PENDING\n",
      "delete completed\n",
      "dataset created....\n",
      "datasetarn:  arn:aws:personalize:us-east-1:247322960887:dataset/aimlbootcamp-20200122/INTERACTIONS\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "dataset_arn = createdataset(datasetPrefix + \"-dataset\", dataset_type, dataset_group_arn, schema_arn)\n",
    "\n",
    "\n",
    "print(\"datasetarn: \", dataset_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time so far:  11.66714859008789\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"time so far: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will do the steps highlighted in red\n",
    "\n",
    "![title](Personalize-create-solution-campaign.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare, Create, and Wait for Dataset Import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach Policy to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicyBootcamp\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicyAIMLBootcamp\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:*\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "policycreateresponse = s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Personalize Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPersonalizeIAMRole(role_name):\n",
    "    iam = boto3.client(\"iam\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "              \"Effect\": \"Allow\",\n",
    "              \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "              },\n",
    "              \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "\n",
    "    #print(\"response: \", response)\n",
    "    #return\n",
    "    try:\n",
    "        create_role_response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "    except Exception as e:\n",
    "        if \"EntityAlreadyExists\".lower() in str(e).lower():\n",
    "            print(\"the role already exists!\")\n",
    "            response = iam.list_roles(\n",
    "                PathPrefix=\"/\",\n",
    "                MaxItems=1000\n",
    "            )\n",
    "            #print(\"all roles: \", response)\n",
    "            for item in response[\"Roles\"]:\n",
    "                if item[\"RoleName\"] == role_name:\n",
    "                    role_arn = item[\"Arn\"]\n",
    "                    break\n",
    "                    \n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    # AmazonPersonalizeFullAccrole_arness provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "    # if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "    # that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "    policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "    iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = policy_arn\n",
    "    )\n",
    "    policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "    iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = policy_arn\n",
    "    )   \n",
    "    print(\"pausing execution to allow for IAM role propagation.....\")\n",
    "    time.sleep(30) # wait to allow IAM role policy attachment to propagate\n",
    "\n",
    "    return role_arn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the role already exists!\n",
      "pausing execution to allow for IAM role propagation.....\n",
      "arn:aws:iam::247322960887:role/PersonalizeRoleAIMLBootcamp-imports-2\n"
     ]
    }
   ],
   "source": [
    "role_name = \"PersonalizeRoleAIMLBootcamp-imports-2\"\n",
    "role_arn = createPersonalizeIAMRole(role_name)\n",
    "#json.dumps(create_dataset_response, indent=2)\n",
    "print(role_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ResponseMetadata": {
        "HTTPHeaders": {
         "connection": "keep-alive",
         "content-length": "122",
         "content-type": "application/x-amz-json-1.1",
         "date": "Thu, 23 Jan 2020 12:58:23 GMT",
         "x-amzn-requestid": "3206c6ef-2679-4a7e-b35b-c57374b6d2f7"
        },
        "HTTPStatusCode": 200,
        "RequestId": "3206c6ef-2679-4a7e-b35b-c57374b6d2f7",
        "RetryAttempts": 0
       },
       "datasetImportJobArn": "arn:aws:personalize:us-east-1:247322960887:dataset-import-job/aimlbootcamp-20200122-import-job-2"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "application/json": {
       "expanded": false
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = datasetPrefix + \"-import-job-2\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "#print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "JSON(create_dataset_import_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Import Job to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time so far:  42.33345341682434\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"time so far: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **** 2020-01-23 13:03:25.411522  DatasetImportJob: CREATE IN_PROGRESS                   \r"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "showme = \" \"\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(showme, datetime.datetime.now(), \" DatasetImportJob: {}\".format(status), \"             \", end='\\r')\n",
    "    showme += \"*\"\n",
    "    if len(showme)> 10:\n",
    "        showme = \" \"\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(3.5719)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Personalize-datagroup-dataset-user-interaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Recipe\n",
    "\n",
    "Personalize comes with predefined recipies that you can choose from depending on your case.\n",
    "\n",
    "https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_recipes_response = personalize.list_recipes()\n",
    "recipe_arn = \"arn:aws:personalize:::recipe/aws-hrnn\" # aws-hrnn selected for demo purposes\n",
    "list_recipes_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSolution(name, dataset_group_arn, recipe_arn):\n",
    "    response = personalize.list_solutions(\n",
    "        datasetGroupArn=dataset_group_arn,\n",
    "        maxResults=100\n",
    "    )\n",
    "    #print(response)\n",
    "    for item in response[\"solutions\"]:\n",
    "        if item[\"name\"] == name:\n",
    "            print(\"solution with the same name already exists. Deleting the existing one.\")\n",
    "            try:\n",
    "                response = personalize.delete_solution(solutionArn=item[\"solutionArn\"])\n",
    "            except Exception as e:\n",
    "                if \"ResourceInUseException\".lower() in str(e).lower():\n",
    "                    print(\"delete failed as resource is in use\")\n",
    "            except:\n",
    "                raise\n",
    "        \n",
    "            return createSolution(name+\"1\", dataset_group_arn, recipe_arn) #delete old one. create new one with a new name \n",
    "            \n",
    "    create_solution_response = personalize.create_solution(\n",
    "        name = name,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        recipeArn = recipe_arn\n",
    "    )\n",
    "\n",
    "    solution_arn = create_solution_response['solutionArn']\n",
    "    #print(json.dumps(create_solution_response, indent=2))\n",
    "    return solution_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_arn = createSolution(datasetPrefix + \"-aimlbootcampExampleSolution\", dataset_group_arn, recipe_arn)\n",
    "print(solution_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn = solution_arn\n",
    ")\n",
    "\n",
    "solution_version_arn = create_solution_version_response['solutionVersionArn']\n",
    "print(json.dumps(create_solution_version_response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behind the scenes the following is happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Personalize-behind-the-scenes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Solution Version to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_solution_version_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = solution_version_arn\n",
    "    )\n",
    "    status = describe_solution_version_response[\"solutionVersion\"][\"status\"]\n",
    "    print(datetime.datetime.now(), \" Solution Version Status: {}\".format(status), \"             \", end='\\r')\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(\"time so far: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the console you should see something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Personalize-solution-version.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Metrics of Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_solution_metrics_response = personalize.get_solution_metrics(\n",
    "    solutionVersionArn = solution_version_arn\n",
    ")\n",
    "JSON(get_solution_metrics_response)\n",
    "#print(json.dumps(get_solution_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createcampaign(name, solutionVersionArn):\n",
    "\n",
    "    response = personalize.list_campaigns(\n",
    "        solutionArn=solutionVersionArn,\n",
    "        maxResults=100\n",
    "    )\n",
    "    #print(response)\n",
    "\n",
    "    for item in response[\"campaigns\"]:\n",
    "        if item[\"name\"] == name:\n",
    "            print(\"campaign already exists (deleting): \", name)\n",
    "            response = personalize.delete_campaign(campaignArn=item[\"campaignArn\"])\n",
    "            return createcampaign(name+\"1\", solutionVersionArn) #recursively create/delete until no more collisions\n",
    "\n",
    "    try:\n",
    "        create_campaign_response = personalize.create_campaign(\n",
    "            name = name,\n",
    "            solutionVersionArn = solution_version_arn,\n",
    "            minProvisionedTPS = 1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"ResourceAlreadyExistsException\".lower() in str(e).lower():\n",
    "            return createcampaign(name+\"1\", solutionVersionArn) #try recursively\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    campaign_arn = create_campaign_response['campaignArn']\n",
    "    #print(json.dumps(create_campaign_response, indent=2))\n",
    "    print(\"created new campaign: \", name, campaign_arn)\n",
    "    return campaign_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Campaign to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_arn = createcampaign(\"aimlbootcampcampaign\", solution_version_arn)\n",
    "print(campaign_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn\n",
    "    )\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    end = time.time()\n",
    "    print(\"Campaign: {}\".format(status), \"time so far: \", end - start)\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a User and an Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(\"time so far (s): \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('./ml-100k/u.item', sep='|', usecols=[0,1], encoding='latin-1')\n",
    "items.columns = ['ITEM_ID', 'TITLE']\n",
    "\n",
    "user_id, item_id, _ = data.sample().values[0]\n",
    "item_title = items.loc[items['ITEM_ID'] == item_id].values[0][-1]\n",
    "print(\"USER: {}\".format(user_id))\n",
    "print(\"ITEM: {}\".format(item_title))\n",
    "\n",
    "#items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call GetRecommendations\n",
    "\n",
    "Note that here we use the Run-time API. The specific method we are calling is this one.\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize-runtime.html#PersonalizeRuntime.Client.get_recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = str(user_id),\n",
    "    itemId = str(item_id)\n",
    ")\n",
    "\n",
    "item_list = get_recommendations_response['itemList']\n",
    "title_list = [items.loc[items['ITEM_ID'] == np.int(item['itemId'])].values[0][-1] for item in item_list]\n",
    "\n",
    "print(\"Recommendations: {}\".format(json.dumps(title_list, indent=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def putevents(userid, trackingid, sessionid, eventlist):\n",
    "    client = boto3.client('personalize-events')\n",
    "    response = client.put_events(\n",
    "        trackingId=trackingid,\n",
    "        userId=userid,\n",
    "        sessionId=sessionid,\n",
    "        eventList=eventlist\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO - implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
